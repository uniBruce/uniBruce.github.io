<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaisiyuan Wang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kaisiyuan Wang</name>
              </p>
              <p>I am currently a research scientist of Baidu VIS. I received my B.S. & M.S. degree from Harbin Institute of Technology, Electrical and Engineering School and Ph.D. degree from the University of Sydney, Electrical and Information Engineering School.
              </p>
              <p>
                Since 2022, I have been a research intern at Department of Computer Vision Technology (VIS), Baidu Inc, working closely with <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a> and <a href="https://liuziwei7.github.io/">Ziwei Liu</a> on both audio/video-driven high-fidelity and efficient human video synthesis techniques. Previously, I also had a pleasant intern experience in Mobile Intelligence Group (MIG), Sensetime working with <a href="https://wywu.github.io/">Wayne Wu</a>, <a href="https://wuqianyi.top/">Qianyi Wu</a> and <a href="https://github.com/jixinya">Xinya Ji</a> on Emotional Talking Face Generation.
              </p>
              <p style="text-align:center">
                <a href="mailto:kaisiyuan.wang@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/KaisiyuanWang_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2Pedf3EAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/uniBruce/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:20%;max-width:20%">
              <a href="images/kaisiyuan_wang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/kaisiyuan_wang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests include both human-centric (e.g., Human Video Synthesis and Co-speech Gesture Generation) and object-centric (e.g., Object-Compositional Implicit Scene Reconstruction) topics. Recently, I am pushing research in Human-Object Interaction (HOI) that can be applied in practical video generation techniques under digital human scenarios.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/rehold.png"><img src="images/rehold.png" alt="Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://fyycs.github.io/Re-HOLD/">
                <papertitle>Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model</papertitle>
              </a>
              <br>
              Yingying Fan,
              <a href="https://orcid.org/0000-0003-3997-0031">Quanwei Yang</a>,
              <strong>Kaisiyuan Wang</strong>&dagger;, 
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              Yingying Li,
              Haocheng Feng,
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://yu-wu.net/">Yu Wu</a>&dagger;,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>
              <br>
              <em>CVPR</em>2025
              <br>
              <a href="https://fyycs.github.io/Re-HOLD/">project page</a>
              /
              <a href="https://arxiv.org/abs/2503.16942">pdf</a>
              /
              <a href="https://fyycs.github.io/Re-HOLD/">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/audcast.png"><img src="images/audcast.png" alt="AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://guanjz20.github.io/projects/AudCast/">
                <papertitle>AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers</papertitle>
              </a>
              <br>
              <a href="https://guanjz20.github.io/">Jiazhi Guan</a>,
              <strong>Kaisiyuan Wang</strong>,
              Zhiliang Xu,
              <a href="https://orcid.org/0000-0003-3997-0031">Quanwei Yang</a>,
              <a href="https://scholar.google.com/citations?user=Vrq1yOEAAAAJ">Yasheng Sun</a>,
              Shengyi He,
              Borong Liang,
              Yingying Li,
              Haocheng Feng,
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,
              <a href="https://www.cs.tsinghua.edu.cn/csen/info/1309/4355.htm">Youjian Zhao</a>&dagger;,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>&dagger;,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>CVPR</em>2025
              <br>
              <a href="https://guanjz20.github.io/projects/AudCast/">project page</a>
              /
              <a href="https://arxiv.org/abs/2503.19824">pdf</a>
              /
              <a href="https://guanjz20.github.io/projects/AudCast/">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/showmaker.png"><img src="images/showmaker.png" alt="ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://orcid.org/0000-0003-3997-0031">
                <papertitle>ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling</papertitle>
              </a>
              <br>
              <a href="https://orcid.org/0000-0003-3997-0031">Quanwei Yang</a>,
              <a href="https://guanjz20.github.io/">Jiazhi Guan</a>,
              <strong>Kaisiyuan Wang</strong>&dagger;, 
              <a href="https://xiaoyun4.github.io/">Lingyun Yu</a>,
              <a href="https://scholar.google.com/citations?user=1Ae0CMgAAAAJ&hl=en">Wenqing Chu</a>,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,
              <a href="https://scholar.google.com/citations?user=lAxit1AAAAAJ&hl=zh-CN">Hongtao Xie</a>&dagger;
              <br>
              <em>NeurlPS</em>2024
              <br>
              <a href="https://orcid.org/0000-0003-3997-0031">project page</a>
              /
              <a href="https://orcid.org/0000-0003-3997-0031">pdf</a>
              /
              <a href="https://orcid.org/0000-0003-3997-0031">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/talkact.png"><img src="images/talkact.png" alt="TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://guanjz20.github.io/projects/TALK-Act/">
                <papertitle>TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model</papertitle>
              </a>
              <br>
              <a href="https://guanjz20.github.io/">Jiazhi Guan</a>,
              Quanwei Yang,
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              Shengyi He,
              Zhiliang Xu,
              Haocheng Feng,
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,
              <a href="https://scholar.google.com/citations?user=lAxit1AAAAAJ&hl=zh-CN">Hongtao Xie</a>,
              <a href="https://www.cs.tsinghua.edu.cn/csen/info/1309/4355.htm">Youjian Zhao</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>Siggraph Asia</em>2024
              <br>
              <a href="https://guanjz20.github.io/projects/TALK-Act/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2410.10696">pdf</a>
              /
              <a href="https://guanjz20.github.io/projects/TALK-Act/">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/resyncer.png"><img src="images/resyncer.png" alt="ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://guanjz20.github.io/projects/ReSyncer/">
                <papertitle>ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer</papertitle>
              </a>
              <br>
              <a href="https://guanjz20.github.io/">Jiazhi Guan</a>,
              Zhiliang Xu,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              <strong>Kaisiyuan Wang</strong>,
              Shengyi He,
              Zhanwang Zhang,
              Borong Liang,
              Haocheng Feng,
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=tVV3jmcAAAAJ&view_op=list_works&sortby=pubdate">Jingtuo Liu</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,
              <a href="https://www.cs.tsinghua.edu.cn/csen/info/1309/4355.htm">Youjian Zhao</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>ECCV</em>2024
              <br>
              <a href="https://guanjz20.github.io/projects/ReSyncer/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2408.03284">pdf</a>
              /
              <a href="https://arxiv.org/pdf/2408.03284">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/radnerf.png"><img src="images/radnerf.png" alt="Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial Decomposition" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://me.kiui.moe/radnerf/">
                <papertitle>Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial Decomposition</papertitle>
              </a>
              <br>
              <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              <a href="https://charlescxk.github.io/">Xiaokang Chen</a>,
              <a href="https://scholar.google.com/citations?user=ui6DYGoAAAAJ">Dongliang He</a>,
              Tianshu Hu,
              <a href="https://scholar.google.com/citations?hl=en&user=tVV3jmcAAAAJ&view_op=list_works&sortby=pubdate">Jingtuo Liu</a>,
              <a href="https://www.cis.pku.edu.cn/info/1177/1378.htm">Gang Zeng</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>
              <br>
              <em>Arxiv</em>
              <br>
              <a href="https://me.kiui.moe/radnerf/">project page</a>
              /
              <a href="https://arxiv.org/abs/2211.12368">pdf</a>
              /
              <a href="https://github.com/ashawkey/RAD-NeRF">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/objsdfplusplus.png"><img src="images/objsdfplusplus.png" alt="Objectsdf++: Improved Object-Compositional Neural Implicit Surfaces" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://wuqianyi.top/">
                <papertitle>Objectsdf++: Improved Object-Compositional Neural Implicit Surfaces</papertitle>
              </a>
              <br>
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://likojack.github.io/kejieli/">Kejie Li</a>
              <a href="https://personal.ntu.edu.sg/asjmzheng/">Jianmin Zheng</a>
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://wuqianyi.top/">project page</a>
              /
              <a href="https://wuqianyi.top/">pdf</a>
              /
              <a href="https://wuqianyi.top/">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/stylesync.png"><img src="images/stylesync.png" alt="StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hangz-nju-cuhk.github.io/projects/StyleSync">
                <papertitle>StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator</papertitle>
              </a>
              <br>
              Jiazhi Guan,
              Zhanwang Zhang,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              Tianshu Hu,
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://scholar.google.com/citations?user=ui6DYGoAAAAJ">Dongliang He</a>,
              Haocheng Feng,
              <a href="https://scholar.google.com/citations?hl=en&user=tVV3jmcAAAAJ&view_op=list_works&sortby=pubdate">Jingtuo Liu</a>,
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://hangz-nju-cuhk.github.io/">project page</a>
              /
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guan_StyleSync_High-Fidelity_Generalized_and_Personalized_Lip_Sync_in_Style-Based_Generator_CVPR_2023_paper.pdf">pdf</a>
              /
              <a href="https://github.com/guanjz20/StyleSync">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/vpgc.png"><img src="images/vpgc.png" alt="Efficient Video Portrait Reenactment via Grid-based Codebook" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/uniBruce/VPGC_Pytorch">
                <papertitle>Efficient Video Portrait Reenactment via Grid-based Codebook</papertitle>
              </a>
              <br>
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
              Tianshu Hu,
              Zhiliang Xu,
              Borong Liang,
              Tianshu Hu, 
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=tVV3jmcAAAAJ&view_op=list_works&sortby=pubdate">Jingtuo Liu</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>
              <br>
              <em>Siggraph</em>, 2023
              <br>
              <a href="https://wuqianyi.top/">project page</a>
              /
              <a href="https://wuqianyi.top/">pdf</a>
              /
              <a href="https://github.com/uniBruce/VPGC_Pytorch">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/vpnq.png"><img src="images/vpnq.png" alt="Robust Video Portrait Reenactment via Personalized Representation Quantization" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/uniBruce/VPNQ_Pytorch">
                <papertitle>Robust Video Portrait Reenactment via Personalized Representation Quantization</papertitle>
              </a>
              <br>
              <strong>Kaisiyuan Wang</strong>,
              Changcheng Liang,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://scholar.google.com/citations?user=ui6DYGoAAAAJ&hl=en">Dongliang He</a>,
              <a href="https://www.linkedin.com/in/zhibin-hong-8275154b/?originalSubdomain=au">Zhibin Hong</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=tVV3jmcAAAAJ&view_op=list_works&sortby=pubdate">Jingtuo Liu</a>,
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>
              <br>
              <em>AAAI</em>, 2023
              <br>
              <a href="https://wuqianyi.top/">project page</a>
              /
              <a href="https://wuqianyi.top/">pdf</a>
              /
              <a href="https://github.com/uniBruce/VPNQ_Pytorch">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:17px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/vpu.png"><img src="images/vpu.png" alt="VPU: A Video-based Point cloud Upsampling framework" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9759233">
                <papertitle>VPU: A Video-based Point cloud Upsampling framework</papertitle>
              </a>
              <br>
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://lucassheng.github.io/">Lu Sheng</a>,
              <a href="https://github.com/ShuhangGu">Shuhang Gu</a>,
              <a href="https://www.cs.hku.hk/index.php/people/academic-staff/dongxu">Dong Xu</a>,
              <br>
              <em>TIP</em>, 2022
              <br>
              <a href="https://github.com/uniBruce">project page</a>
              /
              <a href="https://ieeexplore.ieee.org/document/9759233">pdf</a>
              /
              <a href="https://github.com/uniBruce">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/masklipsync.png"><img src="images/masklipsync.png" alt="Masked lip-sync prediction by audio-visual contextual exploitation in transformers" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hangz-nju-cuhk.github.io/projects/AV-CAT">
                <papertitle>Masked lip-sync prediction by audio-visual contextual exploitation in transformers</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=Vrq1yOEAAAAJ">Yasheng Sun</a>*,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>*,
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://www.linkedin.com/in/zhibin-hong-8275154b/?originalSubdomain=au">Zhibin Hong</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=tVV3jmcAAAAJ&view_op=list_works&sortby=pubdate">Jingtuo Liu</a>,
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ&hl=en">Errui Ding</a>,
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
              <a href="https://www.vogue.cs.titech.ac.jp/koike">Koike Hideki</a>
              
              <br>
              <em>Siggraph Asia</em>, 2022
              <br>
              <a href="https://hangz-nju-cuhk.github.io/projects/AV-CAT">project page</a>
              /
              <a href="https://dl.acm.org/doi/abs/10.1145/3550469.3555393">pdf</a>
              /
              <a href="https://hangz-nju-cuhk.github.io/projects/AV-CAT">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/eamm.png"><img src="images/eamm.png" alt="EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://jixinya.github.io/projects/EAMM/">
                <papertitle>EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model</papertitle>
              </a>
              <br>
              Xinya Ji,
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>,
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://wywu.github.io/">Wayne Wu</a>,
              <a href="http://xufeng.site/">Feng Xu</a>,
              <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>
              <br>
              <em>Siggraph</em>, 2022
              <br>
              <a href="https://jixinya.github.io/projects/EAMM/">project page</a>
              /
              <a href="https://jixinya.github.io/projects/EAMM/resources/EAMM_SIG2022.pdf">pdf</a>
              /
              <a href="https://github.com/jixinya/EAMM">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/evp.png"><img src="images/evp.png" alt="Audio-Driven Emotional Video Portraits" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://jixinya.github.io/projects/EAMM/">
                <papertitle>Audio-Driven Emotional Video Portraits</papertitle>
              </a>
              <br>
              Xinya Ji,
              <a href="https://jixinya.github.io/projects/evp/">Hang Zhou</a>,
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://wywu.github.io/">Wayne Wu</a>,
              <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>,
              <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>,
              <a href="http://xufeng.site/">Feng Xu</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://jixinya.github.io/projects/evp/">project page</a>
              /
              <a href="https://arxiv.org/abs/2104.07452">pdf</a>
              /
              <a href="https://github.com/jixinya/EVP#audio-driven-emotional-video-portraits-cvpr2021">code</a>
            </td>
          </tr>

          <tr> <!-- bgcolor="#ffffd0"-->
            <td style="padding-left:20px;padding-top:7px;padding-bottom:7px;width:25%;vertical-align:middle">
              <a href="images/mead.png"><img src="images/mead.png" alt="MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation" width="200"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://wywu.github.io/projects/MEAD/MEAD.html">
                <papertitle>MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation</papertitle>
              </a>
              <br>
              <strong>Kaisiyuan Wang</strong>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://github.com/Linsen13">Linsen Song</a>,
              <a href="https://yzhq97.github.io/">Zhuoqian Yang</a>,
              <a href="https://wywu.github.io/">Wayne Wu</a>,
              <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=zh-CN">Chen Qian</a>,
              <a href="https://rhe-web.github.io/">Ran He</a>,
              <a href="https://mmlab.siat.ac.cn/yuqiao/">Yu Qiao</a>,
              <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>,
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://wywu.github.io/projects/MEAD/MEAD.html">project page</a>
              /
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660698.pdf">pdf</a>
              /
              <a href="https://github.com/uniBruce/Mead">code</a>
            </td>
          </tr>

        </tbody></table>

				
        <hr class="soft"/>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
          <tr>
            <td>
              <heading>Intern Experience</heading>
              <br>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding-left:40px;padding-top:7px;padding-bottom:17px;width:25%;vertical-align:middle">
              <a href="https://dreameronair.github.io/"/><img src="images/baidu.png" alt="baidu" width="100" ></a>
            </td>
            <td width="75%" valign="center">
              <!--a href="https://www.bearrobotics.ai/">Robotics Software Engineer, Seoul, South Korea</a-->
              <papertitle style="color:gray">
                <big>Research Intern for Digital Human</big>
              </papertitle>
              <papertitle>
                <big> | VIS, Baidu Inc.</big>
              </papertitle>
              <br>
              Beijing, China  |  Feb. 2022 ~ Now
              <ul>
                <li>
                  Personalized Video Portrait Reenactment
                </li>
                <li>
                  Person-agnostic Audio-driven Talking Head Synthesis
                </li>
              </ul>
            </td>
          </tr>

          <tr>
            <td style="padding-left:40px;padding-top:7px;padding-bottom:17px;width:25%;vertical-align:middle">
              <a href="https://www.sensetime.com/en"/><img src="images/sensetime.png" alt="sensetime" width="100" ></a>
            </td>
            <td width="75%" valign="center">
              <!--a href="https://www.bearrobotics.ai/">Robotics Software Engineer, Seoul, South Korea</a-->
              <papertitle style="color:gray">
                <big>Research Intern for Digital Human</big>
              </papertitle>
              <papertitle>
                <big> | MIG, Sensetime</big>
              </papertitle>
              <br>
              Beijing, China  |  Apr. 2019 ~ Jun. 2020
              <ul>
                <li>
                  2D/3D Emotional Facial Expression Generation
                </li>
                <li>
                  Audio-driven Emotional Talking Head Synthesis
                </li>
              </ul>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This website is adapted from Jon Barron's template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
